[data]
train_data = ./data/train/train_all_record-wo-test.jsonl
valid_data = ./data/test/test_dev-set-200.json
test_data =  ./data/test/test_test-set-719.json

[test]
pos_score = 2
k_list = 1,3,5
metric_list = MAP, MRR, R@k, NDCG@k

test_baseline = False
baseline_ids = 1,2,3,4,5,6,7,8,9,10,11,12

test_ours = True
test_specific = None


[encoder]
backbone = bert
shared = True
pooling = avg



[train]
checkpoint = None

epoch = 5
evidence_sample_num = 1

save_step = 1000
logging_step = 100

batch_size = 8
sub_batch_size = 32

optimizer = adamw
grad_accumulate = 1
learning_rate = 1e-5
weight_decay = 0
step_size = 1
lr_multiplier = 1
reader_num = 1
fp16 = False


[simcse_loss]
use = True

negatives_parallel = True
negatives_cross = False

negatives_parallel_single = False

sim_fct = cos
temperature = 0.1

[attention_loss]
use = False
separate_attention_peak = False


[contra_loss]
use = True
rm_simcse = False

positive_attention = True
positive_query = False


query = evidence
value_sample_num = 1

negatives_attention = True
remove_hard_attention = True

negatives_value = False
neg_value_key = double

negatives_query = False
neg_query_key = double
remove_hard_query = False

sim_fct = cos
temperature = 0.1


[attention]
type = dot
scale = 1.0
temperature = 0.1


[positive_weight]
use = True
range = in_batch
normalize = none

source = dot
type = norm
log_sum = False


[output] #output parameters
output_time = 1
test_time = 1

model_path = ./output/unified


[baseline]
pooling = avg

model1 = bert
model2 = bert-tiny
model3 = albert
model4 = roberta
model5 = ernie
model6 = mengzi
model7 = lawformer
model8 = legal-simcse
model9 = sbert

model10 = tfidf
model11 = bm25
model12 = boe

